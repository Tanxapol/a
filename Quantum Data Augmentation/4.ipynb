{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01169a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Training VQC + Classical Decoder...\n",
      "Epoch 0: Loss = 3.6093\n",
      "Epoch 10: Loss = 2.4506\n",
      "Epoch 20: Loss = 2.2735\n",
      "Epoch 30: Loss = 2.1855\n",
      "Epoch 40: Loss = 2.1576\n",
      "Epoch 50: Loss = 2.1408\n",
      "Epoch 60: Loss = 2.1698\n",
      "Epoch 70: Loss = 2.2230\n",
      "Epoch 80: Loss = 2.1678\n",
      "Epoch 90: Loss = 2.1684\n",
      "\n",
      "üéâ Generating synthetic data...\n",
      "   sepal length  sepal width  petal length  petal width                  label\n",
      "0      5.014321     3.425645      1.465074     0.250401  Generated (VQC + MLP)\n",
      "1      5.014321     3.425645      1.465074     0.250401  Generated (VQC + MLP)\n",
      "2      5.014321     3.425645      1.465074     0.250401  Generated (VQC + MLP)\n",
      "3      5.014321     3.425645      1.465074     0.250401  Generated (VQC + MLP)\n",
      "4      5.014321     3.425645      1.465074     0.250401  Generated (VQC + MLP)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import Aer\n",
    "from qiskit.circuit import ParameterVector\n",
    "\n",
    "# === 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Iris ‡πÅ‡∏•‡∏∞ Scaling ===\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X = X[y == 0][:50]  # class Setosa\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á VQC ‡∏î‡πâ‡∏ß‡∏¢ 8 qubits ===\n",
    "n_qubits = 8\n",
    "latent_dim = 8  # Match number of qubits\n",
    "params = ParameterVector(\"Œ∏\", latent_dim)\n",
    "\n",
    "def create_vqc(params):\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    for i in range(n_qubits):\n",
    "        qc.ry(params[i % latent_dim], i)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qc.cx(i, i + 1)\n",
    "    qc.measure_all()\n",
    "    return qc\n",
    "\n",
    "def get_probs(theta, shots=1024):\n",
    "    qc = create_vqc(params)\n",
    "    bound = qc.assign_parameters({params[i]: theta[i] for i in range(latent_dim)})\n",
    "    backend = Aer.get_backend(\"qasm_simulator\")\n",
    "    tqc = transpile(bound, backend)\n",
    "    job = backend.run(tqc, shots=shots)\n",
    "    counts = job.result().get_counts()\n",
    "\n",
    "    probs = np.zeros(2**n_qubits)\n",
    "    for bit, count in counts.items():\n",
    "        idx = int(bit[::-1], 2)  # flip\n",
    "        probs[idx] = count / shots\n",
    "    return probs\n",
    "\n",
    "# === 3. MLP Decoder: 256-dim input ‚Üí 4 features ===\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim=256, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "decoder = Decoder()\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# === 4. ‡πÄ‡∏ó‡∏£‡∏ô VQC + Decoder ‡πÅ‡∏ö‡∏ö Hybrid ===\n",
    "print(\"üß† Training VQC + Classical Decoder...\")\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(X_scaled)):\n",
    "        theta = np.random.uniform(0, 2 * np.pi, latent_dim)\n",
    "        probs = get_probs(theta)\n",
    "\n",
    "        x_in = torch.tensor(probs, dtype=torch.float32)\n",
    "        y_true = torch.tensor(X_scaled[i], dtype=torch.float32)\n",
    "\n",
    "        y_pred = decoder(x_in)\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {total_loss:.4f}\")\n",
    "\n",
    "# === 5. Generate new synthetic Iris data ===\n",
    "print(\"\\nüéâ Generating synthetic data...\")\n",
    "generated_data = []\n",
    "for _ in range(200):\n",
    "    theta = np.random.uniform(0, 2 * np.pi, latent_dim)\n",
    "    probs = get_probs(theta)\n",
    "    x_in = torch.tensor(probs, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        out = decoder(x_in).numpy()\n",
    "    generated_data.append(out)\n",
    "\n",
    "generated_data = scaler.inverse_transform(np.array(generated_data))\n",
    "\n",
    "# === 6. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(generated_data, columns=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"])\n",
    "df[\"label\"] = \"Generated (VQC + MLP)\"\n",
    "print(df.head())\n",
    "\n",
    "# === Save to CSV\n",
    "df.to_csv(\"4_generated_iris.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
